# TCC-ml

Essas quatro m√©tricas ‚Äì **Acur√°cia, Precis√£o, Revoca√ß√£o (Recall) e F1-Score** ‚Äì s√£o fundamentais para avaliar o desempenho de um modelo de classifica√ß√£o. Vamos entender cada uma:  

---

### **1Ô∏è‚É£ Acur√°cia (Accuracy)**
**Mede a propor√ß√£o de previs√µes corretas em rela√ß√£o ao total de previs√µes.**  

\[
\text{Acur√°cia} = \frac{\text{N¬∫ de previs√µes corretas}}{\text{Total de amostras}}
\]

‚úÖ **Vantagem:** Funciona bem quando as classes est√£o balanceadas.  
‚ùå **Problema:** Pode ser enganosa em **dados desbalanceados**. Exemplo: Se 95% dos dados pertencem √† classe A, um modelo que sempre prev√™ A ter√° 95% de acur√°cia, mas n√£o ser√° √∫til.  

üìå **Exemplo:**  
Se temos **100 amostras** e o modelo acerta **90**, a acur√°cia √© **90%**.  

---

### **2Ô∏è‚É£ Precis√£o (Precision)**
**Mede quantas das previs√µes positivas estavam corretas.**  

\[
\text{Precis√£o} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
\]

‚úÖ **Importante quando os falsos positivos s√£o cr√≠ticos.**  
üìå **Exemplo:** Em um teste para detectar **c√¢ncer**, um falso positivo pode causar p√¢nico desnecess√°rio.  

üîπ **Interpreta√ß√£o**:  
- **Alta precis√£o** ‚Üí Poucos falsos positivos.  
- **Baixa precis√£o** ‚Üí Muitos falsos positivos.  

---

### **3Ô∏è‚É£ Revoca√ß√£o (Recall) (Sensibilidade ou Sensibilidade Verdadeira)**
**Mede quantas das inst√¢ncias positivas foram corretamente identificadas.**  

\[
\text{Revoca√ß√£o} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
\]

‚úÖ **Importante quando os falsos negativos s√£o cr√≠ticos.**  
üìå **Exemplo:** Em um teste para **doen√ßa grave**, um falso negativo significa que um paciente doente n√£o ser√° tratado!  

üîπ **Interpreta√ß√£o**:  
- **Alta revoca√ß√£o** ‚Üí Poucos falsos negativos (√≥timo para detectar doen√ßas).  
- **Baixa revoca√ß√£o** ‚Üí Muitos falsos negativos.  

---

### **4Ô∏è‚É£ F1-Score (M√©dia Harm√¥nica entre Precis√£o e Recall)**
**Combina Precis√£o e Recall em uma √∫nica m√©trica balanceada.**  

\[
\text{F1-Score} = 2 \times \frac{\text{Precis√£o} \times \text{Revoca√ß√£o}}{\text{Precis√£o} + \text{Revoca√ß√£o}}
\]

‚úÖ **√ötil quando h√° desequil√≠brio entre classes.**  
üìå **Exemplo:** Se um modelo tem **alta precis√£o mas baixa revoca√ß√£o**, ou vice-versa, o F1-Score ajudar√° a encontrar um equil√≠brio.  

üîπ **Interpreta√ß√£o**:  
- **F1 pr√≥ximo de 1** ‚Üí Bom equil√≠brio entre precis√£o e recall.  
- **F1 baixo** ‚Üí O modelo tem problemas com precis√£o ou recall.  

---

### **Resumo das Diferen√ßas**
| M√©trica        | Pergunta que responde | Quando usar? |
|---------------|----------------------|-------------|
| **Acur√°cia**  | "Quantos acertos o modelo fez no total?" | Se as classes est√£o **balanceadas**. |
| **Precis√£o**  | "Dos positivos que o modelo previu, quantos estavam certos?" | Quando **falsos positivos** s√£o mais graves. |
| **Recall**    | "Dos casos realmente positivos, quantos o modelo encontrou?" | Quando **falsos negativos** s√£o mais graves. |
| **F1-Score**  | "Existe equil√≠brio entre precis√£o e recall?" | Quando h√° **desbalanceamento entre classes**. |

---

### **Exemplo Pr√°tico**
Vamos supor um modelo de **detec√ß√£o de spam**, onde:  
- **TP (True Positives)** ‚Üí E-mails corretamente classificados como spam.  
- **FP (False Positives)** ‚Üí E-mails bons classificados incorretamente como spam.  
- **FN (False Negatives)** ‚Üí E-mails de spam n√£o detectados.  

Se tivermos:  
‚úÖ **Alta precis√£o** ‚Üí Poucos e-mails bons sendo marcados como spam.  
‚úÖ **Alta revoca√ß√£o** ‚Üí A maioria dos spams s√£o detectados corretamente.  
‚û° **F1-Score** nos dir√° se o modelo tem um bom equil√≠brio entre esses dois fatores.  

Se precisar de c√≥digo em **Python** para calcular essas m√©tricas, me avise! üöÄ